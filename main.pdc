---
title: "Tackling the Reproducibility Problem in Systems Research with Declarative Experiment Specifications"
author:
  - name: "Ivo Jimenez, Carlos Maltzahn"
    affiliation: "UC Santa Cruz"
    email: "`{ivo,carlosm}@cs.ucsc.edu`"
  - name: "Jay Lofstead"
    affiliation: "Sandia National Lab"
    email: "`gflofst@sandia.gov`"
  - name: "Adam Moody, Kathryn Mohror"
    affiliation: "Lawrence Livermore National Lab"
    email: "`{moody20,kathryn}@llnl.gov`"
abstract: |
  Validating experimental results in the field of computer systems is 
  a challenging task, mainly due to the many changes in software and 
  hardware that computational environments go through. Determining if 
  an experiment is reproducible entails two separate tasks: 
  re-executing the experiment and validating the results. Existing 
  reproducibility efforts have focused in the former, envisioning 
  techniques and infrastructures that makes it easier to re-execute an 
  experiment. In this work we focus on the latter by analyzing the 
  validation workflow that an experiment re-executioner goes through. 
  We notice that validating results is done on the basis of experiment 
  design and high-level goals, rather than exact quantitative metrics. 
  Based on this insight, we introduce a declarative format for 
  specifying the high-level components of an experiment as well as 
  describing generic, testable conditions that serve as the basis for 
  validation. We present a use case in the area of storage systems to 
  illustrate the usefulness of this approach. We also discuss 
  limitations and potential benefits of using this approach in other 
  areas of experimental systems research.
tags:
  - vision-paper
  - socc15
category: pubs
layout: paper
fontfamily: times
classoption:
  - 10pt
  - reprint
numbersections: true
documentclass: sigplanconf
substitute-hyperref: true
monofont-size: scriptsize
sigplanconf: true
links-as-notes: true
csl: "ieee.csl"
bibliography: "citations.bib"
usedefaultspacing: yes
keywords:
  - reproducibility
categories:
  - category: "D.3.2"
    subcategory: "Language Classifications"
    third: "Very high-level languages"
  - category: "D.4.m"
    subcategory: "Operating Systems"
    third: "Miscellaneous"
  - category: "K.7.3"
    subcategory: "The Computing Profession"
    third: "General"
---

# Introduction

A key component of the scientific method is the ability to revisit and 
replicate previous experiments. Registering detailed information about 
how an experiment was carried out allows scientists to interpret and 
understand results, as well as verify that the experiment was 
performed according to acceptable procedures. Additionally, 
reproducibility plays a major role in education since the amount of 
information that a student has to digest increases as the pace of 
scientific discovery accelerates. By having the ability to reproduce 
experiments, a student can learn by looking at provenance information, 
re-evaluate the questions that the original experiment answered and 
thus "stand on the shoulder of giants".

In applied computer science an experiment is carried out in its 
entirety on a computer. Thus, in principle, a well documented 
experiment should be reproducible automatically (e.g., by typing 
`make`); however, this is not the case. Today's computational 
environments are complex and accounting for all possible effects of 
changes within and across systems is a challenging task. This and 
other related issues have gained substantial attention lately in 
systems research [@vitek_repeatability_2011 ; 
@collberg_repeatability_2015 ; @dietrich_dataref_2015 ; 
@feitelson_repeatability_2015], computational science 
[@neylon_changing_2012 ; @peng_reproducible_2011 ; 
@freire_computational_2012 ; @donoho_reproducible_2009] and science in 
general [@achenbach_new_2015 ; @yaffe_reproducibility_2015 ; 
@editorial_journals_2014]. The role of computers in new scientific 
discoveries has only increased and will continue. Coupled with 
this, the advent of cloud computing offers new opportunities for experiment
validation. Cloud computing makes it easier to share code and 
data simplifying collaboration for implementating experiments.
While it is becoming easier to collaborate, the same 
cannot be said about experiment validation.
Even in the cloud computing scenario, there is a serious lack of tools 
that help researchers identify when an experiment is generating 
valid results.  The goal of our work is to close this gap in the area of
systems research and storage systems in particular.

When discussing reproducibility, the terms _reproducibility_, 
_repeatability_, _replicability_ and _recomputability_ (among many 
others) are often used, sometimes interchangeably. In our work we only 
employ _repeatability_ and _reproducibility_. We borrow and refine the 
definitions introduced by Vitek et al. [@vitek_repeatability_2011]:

> Repeatability. _The ability to re-run the exact same experiment with 
> the same method on the same or similar system and obtain the same or 
> very similar result._
>
> Reproducibility. _The independent confirmation of a scientific 
> hypothesis through reproduction by an independent researcher/lab. 
> The reproductions are carried out after a publication, based on the 
> information in the paper and possibly some other information, such 
> as data sets, published via scientific data repositories or provided 
> by the authors on inquiry_.

The reproduction of an experiment can be seen as being composed of (1) 
its execution and (2) the validation of the results. Generally, these 
two tasks are conflated when designating an experiment as 
reproducible. In our case, we treat re-execution and validation 
separately since it facilitates the discussion and makes it easer to 
identify the missing pieces in terms of a generic, systematic approach 
to reproducibility.

Regarding re-execution, the line that divides repeatability from 
reproducibility can be subjective; from the definition of 
repeatability, the phrase _exact same experiment with the same method 
on the same or similar system_ can be a source of debate. To minimize 
confusion, we concretize it by defining repeatability with respect to the 
components of an experiment. In general, an experiment is composed of 
a relatively complex computational environment that includes one or 
more of the following: hardware, virtualization, OS, configuration, 
code, data and workload ([see\ ](#sec:means)). We refer to these 
components as the _means_ of an experiment. For each of these, we can 
determine precisely whether they are the same as in the original 
experiment or not. We say that we repeat an experiment only if the 
means are the same as in the original execution. If any of them has 
changed and we re-execute the experiment, we say we are now trying to 
reproduce the original execution.

Version-control systems (VCS) are sometimes used to ease the 
recreation of an experimental environment. Having a specific version 
ID for the source code and data associated with an article on a public 
repository significantly reduces the burden of recreating the means of 
an experiment [@brown_how_2014]. However, availability of the source 
code does not guarantee reproducibility [@collberg_repeatability_2015] 
since the code might not compile and, even if compilable, the 
resulting program might not generate the same results. Recreating an 
environment that resembles the one where an experiment was originally 
executed is a challenging task [@krishnamurthi_real_2015]. 
Virtualization technologies can play a big role in accomplishing this 
[@clark_xen_2004 ; @jimenez_role_2015]. In the end, if required, the 
means of an experiment can be audited by experts to determine 
whether or not they sufficiently resemble the original ([](#sec:workflow)).

The second step in determining the reproducibility of an experiment is 
validating the results of an execution. Regardless of whether an 
execution is repeated or reproduced, at this point, the reviewer has 
to answer the question: _"are these results validating the original 
ones?"_ An alternative validation criterion can rely on the exact 
quantitative observations, that is, results validate the original work 
if the exact same numerical values of the original output are 
obtained. This should be applicable for an experiment being 
repeated since, per definition, the exact same experiment is executed 
in the same environment. However, given the complex and continual 
changes typical in computational environments, taking exact 
quantitative metrics as the basis for validation might be problematic
since more often than not an experiment execution will be reproduced 
rather than repeated. In this case, the validation leeway is 
very limited. Thus, ideally, we would like to have a way of specifying 
validation criteria that are as independent as possible from the 
particular implementation details, i.e., a way of testing the validity 
of the original work agnostic to the means of the experiment. 
We propose to take experiment goals as the basis for validation and 
treat observations in the context of these goals.

# Goals, Means, and Observations {#sec:goals-means-obs}

The high-level structure of an experiment can be described as having 
three components: goals, means, and observations. Two additional transient
components, output data and result visualizations, are created as part of
running the experiment and are used as a basis for observations (Figure 1). An
experiment is designed with a particular **goal** in mind. For example, to show
that under certain circumstances, a new system or algorithm improves the
state-of-the-art by an order of magnitude. The **means** of the experiment, as
we mentioned, are the particularities of how the experimental environment 
and procedures are carried out. As part of the experiment execution, 
metrics are collected into an output dataset. This raw 
data can optionally be summarized (e.g., with statistical descriptors) 
before being displayed in a figure and described in the form of 
observations made in the prose of the article. The **observations** 
made about the output data properties are the basis on 
which an author proves and corroborates the hypothesis of her work.

![High-level structure of an experiment.](figures/goals.png)

Declarative formats provide a way to 
express, at a high-level, the relationship among the goals 
and means of an experiment, the raw data it produces, and the figures 
and observations discussed in the article. In other words, it 
enables the author to provide an experiment design description,
its means of execution, and the expected 
observations that validate the author's claims. Such a description 
serves two purposes. First, a reviewer or reader with access to 
this description can, on her own, validate the original work by 
testing whether the original observations hold on the re-generated 
output data. Secondly, the explicit specification of these high-level 
elements aid an author in enhancing the design and presentation of the 
experimental evaluation of a system by forcing her to think about all aspects
of the experiment rather than just generating results for a paper.

# Experiment Specification Format {#sec:format}

An experiment specification format (ESF) allows a scientist to 
explicitly and declaratively capture an experiment's high-level structure.
An example JSON file is shown below. The 
example corresponds to a simplified version of the specification of an 
already-published article ([see\ ](#sec:case)). We describe each 
section of the ESF next.

~~~{.json .numberLines}
{
  "goal_location": {
     "sec": "6.1",
     "par": 5
  },
  "goal_text": "demonstrate that Ceph scales linearly
     with the size of the cluster",
  "goal_category": ["proof_of_concept"],
  "experiments":[ {
     "reference":"figure-8",
     "name":"scalability experiment",
     "tags":["throughput"],
     "hardware_dependencies": [{
        "type": "hdd",
        "bw": "58MB/s"
     }, {
        "type": "network",
        "bw": "1GbE"
     }],
     "software_dependencies": [{
       "type": "os",
       "kernel": "linux 2.6.32",
       "distro": "debian 6.0"
     }, {
       "type": "storage",
       "name": "ceph",
       "version": "0.1.67"
     }],
     "independent_variables": [{
       "type": "method",
       "values": ["raw", "ceph"],
       "desc": "raw corresponds to hdd sequential write
          performance, expressed in MB/s"
     }, {
       "type": "size",
       "values": ["2-24", 2]
     }],
     "dependent_variable": {
       "type":  "throughput",
       "scale": "mb/s"
     },
     "statistical_functions": {
       "functions": ["avg", "stddev"],
       "repetitions": 10
     },
     "validations": [
       "for    size=*
        expect ceph >= (raw * 0.9)"
     ]
  }]
}
~~~

## Experiment Goals {#sec:goals}

There are many distinct types of experiment goals, i.e., the reasoning 
behind a particular experimental design. For example:

  * _Proof of concept_. Show that the idea/algorithm/system works as 
    described. For example, a scalability experiment demonstrating 
    the system scales linearly.
  * _Decision-guiding_. Determine whether a behavior is obtrusive 
    enough that it must be addressed as part of the system design.
    For example, test if data locality in a NUMA-based 
    system is significant so that the code can be optimized to 
    minimize data movement between local and remote memory banks.
  * _Robustness check_. Measure the system's ability to react to 
    changes in the environment. For example, an experiment that 
    executes the same workload on distinct platforms and shows similar 
    behavior on all of them.
  * _Overhead quantification_. When overheads are expected, show how 
    big they are for a particular setup. For example, if a FUSE driver 
    is implemented, measure its overhead against a kernel-level file 
    system module.

This list is not exhaustive, of course, but representative of 
many experiments found in systems research literature. The first 
elements in the ESF specify the experimental goal and link it with one 
or more experiments that appear in the article (lines 2-8) that serve 
to accomplish the goal.

## Means of an Experiment {#sec:means}

<!-- **TODO**: Extend it. Talk about non-determinism -->

While computational systems are complex, advances in version-control 
and cloud computing technologies make it easier to recreate the 
environment on which an experiment runs. Immutability makes it easier 
to fix a large majority of the components of an experiment as well as 
infer and package its dependencies [@chirigati_reprozip_2013]. For 
those components that cannot be fixed to a particular state, tools can 
automatically obtain and format detailed information about the state 
of the execution platform, making it easier to compare between 
original and re-execution environments. The challenge lies in finding, 
when present, the root cause(s) of the differences in original and 
reproduced results [@jimenez_redo_2014].

The ESF contains a section to specify the means of the experiment. In 
the example, this corresponds to lines 13-28 This is a simplified list 
of dependencies for the experiment, used only to illustrate the type 
of information that is captured in this section. A real example would 
be more comprehensive, potentially relying on tools that obtain this 
information [automatically](https://github.com/sosreport/sos).

## Schema of Raw Data {#sec:schema}

While it is important to capture the output data, making it part of 
the ESF would be cumbersome and, as has been mentioned, exact 
numerical repeatability is a very limited validation criterion. 
Instead, it is preferable to have a description of the metrics being 
captured, i.e., the metadata of the experiment's output. For example, 
if the measurements are stored in a CSV file, the experiment 
specification should include the metadata of each column such as name, 
aliases, types, scales and ranges.

The ESF has two entries for independent and dependent variables that 
are used to specify the schema of the output data (lines 29-41). The 
latter refers to the metric being captured while the former 
corresponds to the values over which the measurements are taken. 
Additionally, if statistical functions are applied to the raw data, 
these should also be specified (lines 42-45), along with the number of 
experiment repetitions and sumarization techniques used, if any.

## Observations and Validation Clauses {#sec:validation}

We propose using a declarative language for codifying observations. 
Such a language provides an author with a mechanism to succinctly 
write descriptive statements that can be used to test for 
reproducibility. The simplified syntax for the language is the 
following:

```yaml
  validation
   : 'for' condition ('and' condition)*
     'expect' result ('and' result)*
   ;
  condition
   : vars ('in' range)
   | vars ('='|'<'|'>'|'!=') value
   ;
  result
   : condition
   ;
  vars
   : var (',' var)*
   ;
  range
   : 'between' value 'and' value
   | '[' value (',' value)* ']'
   ;
  value
   : '*' | NUMBER | var '*' NUMBER
   ;
  var
   : STRING
   ;
```

The statements constructed via this language refer to elements on the 
schema of the output data. In other words, the schema specification 
that precedes the `validations` section of the ESF introduces 
syntactic elements into the language that provide an easy way to write 
validation statements. For example, suppose there is an experiment 
that evaluates concurrency control methods and the experiment measures 
their performance while varying the number of worker threads. The 
schema for such an experiment might be the following:

```json
  {
    "independent_variables": [ {
      "type": "method",
      "values": ["baseline", "mine"]
    }, {
      "type": "threads",
      "values": ["2", "4", "8", "16"]
    }],
    "dependent_variable": {
      "type":  "throughput",
      "scale": "ops/s"
    }
  }
```

A statement for this experiment might be:

```sql
  for    threads > 4
  expect mine = (10 * baseline)
```

In prose form, the above describes that when the number of worker 
threads goes beyond 4, `mine` outperforms `baseline` by an order of 
magnitude. When re-executing this experiment, the data should reflect 
this behavior in order to validate the results.

# Case Study {#sec:case}

We illustrate our approach by taking a published paper and describing the
goals, means, and observations, including the validation clauses, that define
the reproducibility criteria. We take the Ceph OSDI '06 paper 
[@weil_ceph_2006] and reproduce one of its experiments. In particular, 
we look at the scalability experiment from the data performance 
section (6.1 on the original paper). The reason for selecting this 
paper is that we are familiar with these experiments making it easier 
to reason about contextual information not necessarily available 
directly from the paper.

The experiments in Section 6.1 of the original paper show the 
Ceph can saturate disk evenly among the cluster's drives 
as well as scale with the size of the storage cluster. Scalability experiment
results are presented in Section 6.1.3 of the 
Ceph paper. The goal of this experiment is to show that Ceph scales 
linearly with the number of storage nodes assuming the
network switch is never saturated. This linear scalability is the validation 
criteria for this experiment and thus what we would like to capture in 
the specification. The network switch capacity is a function of the environment
and may ultimately affect the experiment results.

The experiment used 4 MB objects to minimize random I/O noise from the 
hard drives. We ignore the performance of the `hash` data distribution 
and increase the number of placement groups to 128 per node, thus we 
meaningfully compare against the red solid-dotted line in Figure 8 of 
the Ceph paper, reprinted in Figure 2. The original scalability 
experiment ran with 20 clients per node on 20 nodes (400 clients 
total) and varied the number of object storage devices (OSDs) from 
2-26 in increments of 2. Every node was connected via a 1 GbE link 
yielding a theoretical upper bound of 2GB/s when there 
was enough capacity of the OSD cluster to have 20 1Gb connections or 
alternatively when the connection limit of the switch was reached. The 
paper experiments were executed on a Netgear switch. This device has a 
capacity of approximately 14 GbE in _real_ total traffic (from a 20 
advertised), corresponding to the 24 * 58 = 1400 MB/s combined 
throughput shown in the original paper.

![Reprinting Figure 8 from the original paper [@weil_ceph_2006]. The 
original caption reads: "_object storage device (OSD) write 
performance scales linearly with the size of the OSD cluster until the 
switch is saturated at 24 OSDs. CRUSH and hash performance improves 
when more PGs lower variance in OSD 
utilization_."](figures/figure8.png)

We scaled down the experiment by reducing the number of client nodes 
to 1 running 16 client threads. This means that our network upper 
bound is approximately 110 MB/s, the capacity of the 1GbE link from 
the client to the switch. We throttle I/O to 15 MB/s for each storage 
node [^throttle]. We use this per-OSD increment as our scaling unit.
Figure 3 shows results of the scaled-down, throttled scalability experiment.

[^throttle]: We throttle I/O with the purpose of slowing down the 
experiment. The hard drives used for the reproduced experiment can 
perform at 120 MB/s which would saturate the network link rapidly.

Our experiment demonstrates that Ceph scales linearly with the number of OSDs
until it saturates the 1GbE link of the client at 8 OSDs. The 
sample experiment specification shown earlier ([](#sec:format)) 
corresponds to this experiment. The validation clause in lines 46-48 
specifies the reproducibility criteria for this experiment.[^netio] As 
can be noted, this is where the declarative 
specification stands out since the validation is independent of the 
particularities of the means of each experiment. Even though the 
recreated environment is significantly different from the 
original,[^setup] we are able to reproduce the results by validating 
on the basis of the experiment goal, schema of the output and 
validation clauses expressed as relative rather than absolute throughput
measurements.

[^setup]: Besides scaling down and throttling I/O, the underlying hardware
specifications are significantly different. The original experiment ran on
nodes with a 2-core 2212 AMD Opteron @2.0GHz, 8GB RAM, Linux 2.6.9, 250GB
Seagate Barracuda ES (ST3250620NS) hard disk drive and a very early version of
the Ceph codebase (commit done in 2005). The reproduced experiment ran on nodes
with a 6-core Xeon E5-2630 @2.3GHz, 64GB RAM, 500GB HP6G (658071-B21) hard disk
drive and Ceph version 0.87.1. The same network switch was used for both
experiments, a Netgear GS748T. The complete platform specs are 
contained in the repository associated to this paper 
<https://github.com/ivotron/socc15>

[^netio]: An additional clause might capture the fact that the 
scalability behavior is seen until the network bandwidth is saturated. 
For example, `for size > 24 expect ceph < raw`.

![Reproducing a scaled-down version of the original OSDI '06 
scalability experiment. The x-axis corresponds to the size of the 
cluster (in number of OSDs). The y-axis represents normalized 
throughput (to meaningfully compare against original results) with respect to 
the raw performance of the hard disk drives in the cluster. The red 
line corresponds to the original results and the green line to the one 
generated by the re-execution of the experiment.](figures/ceph.png)

# Discussion

## Usability

Given that the high-level components ([](#sec:goals-means-obs)) 
abstract a large number of experiments that people usually implement 
in the storage systems literature and since this is what a researcher 
usually goes through anyway, creating the specification for an 
experiment represents little extra effort. The exception being documenting the 
experiment means which, as we mentioned before ([](#sec:means)), is a task that
can be automated using currently available tools.

## Integration into Existing Infrastructure

Experimental platforms such as CloudLab [@ricci_introducing_2014] can 
incorporate the notion of _execution_ so that for every experiment a 
record of executions is maintained. For each execution, the means 
section of the ESF can be automatically populated. Validation 
statements can also provide another testability layer for continuous 
integration (CI) systems such as Jenkins.

## Codified Observations As Falsifiable Statements

Validation clauses serve to succinctly codify observations. Given the 
descriptive language design, validation ranges have 
to be provided for each observation so that it can be tested. This has 
the implication of turning observations into falsifiable statements 
[@popper_logic_2002]. These validation clauses are conditions that 
should hold in order to corroborate the conclusions of the paper. In 
other words, if the means of the experiment are properly recreated, 
the specified behavior should be observed.

Experiment goals ([](#sec:goals)) set the tone in which these 
falsifiable statements are treated. For an experiment that proves a 
concept or design, a validation clause has more weight than, say, an 
experiment that quantifies an expected overhead. For example, a system 
that claims to achieve linear scalability, the corresponding 
validation clauses are more significant than those for an experiment 
that shows the overhead of a new file system implemented as a FUSE 
module. In the former, a failed validation invalidates the whole work 
while in the latter the failed test invalidates a minor aspect. In 
other words, some experiments evaluate a high-level claim while others 
refer to low-level aspects, hence the importance of looking at 
experiment goals while looking at validations; goals set the mindset 
of the reader or reviewer that validates the work whenever she 
encounters failed validations. This is the main motivation for having 
goals as an explicit entry on the ESF.

## The Validation Workflow {#sec:workflow}

The ESF has the structure of a conditional statement: given the goals 
and means of an experiment, the observations on the output data should 
hold. Thus, if the validation statements are false with respect to the output 
data of the re-execution of an experiment, it is either because the 
differences between the means of the original and reproduced 
experiment are significantly different, or the original claims cannot 
be corroborated. Thus, before one can determine the latter, one has to 
audit the differences between the means of experimentation and account 
for all of them (Figure 4).

![Validation workflow.](figures/workflow.png)

## Early Feedback

The following are quotes from authors that have kindly worked with us 
by creating specifications for one or more of their published 
experiments:

> Author 1: _Writing an experiment specification makes you think 
> clearly about the overall experiment design_.

> Author 2: _The ESF provides a nice template for carrying out 
> experiments_.

> Author 3: _This approach helps to find meaningful baselines. 
> Reporting raw numbers in figures and observations makes it harder to 
> validate results. Specifying validation clauses respective to 
> baselines and normalized values makes it easier to report 
> reproducible results_.

In general, we have noticed that the exercise of explicitly specifying 
the validation criteria creates a feedback loop in an author's mind 
that results in insightful ideas for experiment design, 
baseline selection, and validation criteria. Additionally, the author's point
of view is explicitly expressed. Usually, figures contain 
more information than necessary to back a claim. 
This which might lead readers to draw other conclusions. Lastly, every 
article has an implicit temporal context associated to it that the 
reader has to keep in mind, for example, the bottleneck at the time 
that an article was published might be in storage devices (e.g., hard 
disk drives) while at other times they might have moved to the network 
instead (e.g., because of the availability of faster storage devices 
such as SSDs). A possibility would be to create a knowledge base that 
an author can link the paper to so that a semantic context is 
available to the reader.

# Related Work {#sec:related}

The challenging task of evaluating experimental results in applied 
computer science has been long recognized [@ignizio_establishment_1971 
; @ignizio_validating_1973 ; @crowder_reporting_1979]. This issue has 
recently received a significant amount of attention from the 
computational research community [@freire_computational_2012 ; 
@neylon_changing_2012 ; @leveqije_reproducible_2012 ; 
@stodden_implementing_2014], where the focus is more on numerical 
reproducibility rather than performance evaluation. Similarly, efforts 
such as _The Recomputation Manifesto_ [@gent_recomputation_2013] and 
the _Software Sustainability Institute_ [@crouch_software_2013] have 
reproducibility as a central part of their endeavour but leave 
performance as a secondary problem. In systems research, performance 
_is_ the subject of study, thus we need to look it as a primary issue.

In [@collberg_repeatability_2015] the authors took 601 articles 
published in 13 top-tier systems research conferences and found that 
32.3% of associated experiments are repeatable (under their definition 
of _weak repeatability_). The authors did not validate the original 
results. In our case, we are interested not only in being able to 
rebuild and execute binaries (repeat/reproduce the execution) but also 
in validating the original claims by testing falsifiable statements on 
the output of the experiment.

Krishnamurthi and Vitek [@krishnamurthi_real_2015] emphasize the 
importance of repeatability and describe recent efforts by the systems 
research community to encourage the submission of experiment artifacts 
as assets associated to an article. We see our work as complementary 
to these since an experiment specification could also be part of this 
list of assets, making it easier to validate a re-generated result.

# Conclusion and Future Work

In the words of Karl Popper: "_the criterion of the scientific status 
of a theory is its falsifiability, or refutability, or testability_". 
By providing a way to specify the high-level components of an 
experiment and validation clauses for observed metrics we effectively 
incorporate falsifiability to the field of experimental storage 
systems. We are in the process of studying the viability of the ESF on 
experiments from other areas of systems research. As part of our work, 
we are working with colleagues in our field to create descriptions for 
already-published experiments and analyze them to check if they 
capture the appropriate validation criteria. While we envision our 
findings to be applicable in the area of systems research, we plan to 
evaluate its suitability on other areas of computer science.

**Acknowledgements:** This work was performed under the auspices of 
the U.S. Department of Energy by Lawrence Livermore National 
Laboratory under Contract DE-AC52-07NA27344. LLNL-CONF-669866-DRAFT.
Sandia National Laboratories is a multi-program laboratory managed and operated
by Sandia Corporation, a wholly owned subsidiary of Lockheed Martin
Corporation, for the U.S. Department of Energy's National Nuclear Security
Administration under contract DE-AC04-94AL85000.

# References

<!-- hanged biblio -->

\noindent
\vspace{-2em}
\setlength{\parindent}{-0.26in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
