---
title: "Tackling the Reproducibility Problem in Systems Research with Declarative Experiment Specifications"
author:
  - name: "Ivo Jimenez \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ Carlos Maltzahn"
    affiliation: "UC Santa Cruz"
    email: "`{ivo,carlosm}@cs.ucsc.edu`"
  - name: "Jay Lofstead"
    affiliation: "Sandia National Lab"
    email: "`gflofst@sandia.gov`"
  - name: "Adam Moody \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ Kathryn Mohror"
    affiliation: "Lawrence Livermore National Lab"
    email: "`{moody11,kathryn}@llnl.gov`"
abstract: |
  Evaluating experimental results in the field of computer systems is 
  a challenging task, mainly due to the many changes in software and 
  hardware that computational environments go through. In this paper, 
  we introduce a declarative format for specifying the goals of an 
  experiment as well as describing testeable observations that serve 
  as the basis for validation. We present a use case in the area of 
  storage systems to illustrate the usefulness of this approach. We 
  also discuss limitations and potential benefits of using this 
  approach in other areas of experimental systems research.
tags:
  - vision-paper
  - socc15
category: pubs
layout: paper
fontfamily: times
classoption:
  - 10pt
  - reprint
numbersections: true
documentclass: sigplanconf
substitute-hyperref: true
sigplanconf: true
links-as-notes: true
csl: "ieee.csl"
bibliography: "citations.bib"
usedefaultspacing: yes
keywords:
  - reproducibility
---

# Introduction

A key component of the scientific method is the ability to revisit and 
replicate previous experiments. Registering detailed information about 
how an experiment was carried out allows scientists to interpret and 
understand results, as well as verify that the experiment was 
performed according to acceptable procedures. Additionally, 
reproducibility plays a major role in education since the amount of 
information that a student has to digest increases as the pace of 
scientific discovery accelerates. By having the ability to repeat 
experiments, a student can learn by looking at provenance information, 
re-evaluate the questions that the original experiment answered and 
thus "stand on the shoulder of giants".

In applied computer science an experiment is carried out in its 
entirety on a computer. Repeating an experiment doesn't require a 
scientist to rewrite a program, rather it entails obtaining the 
original program and executing it (possibly in a distinct 
environment). Thus, in principle, a well documented experiment should 
be repeatable automatically (e.g. by typing `make`); however, this is 
not the case. Today's computational environments are complex and 
accounting for all possible effects of changes within and across 
systems is a challenging task [@freire_computational_2012 ; 
@donoho_reproducible_2009].

The terms _reproducibility_, _repeatability_, _replicability_ and 
_recomputability_ (among many others) are often used, sometimes 
interchangeably, when discussing reproducibility in general. In our 
work we only employ _repeatability_ and _reproducibility_. We borrow 
and refine the definitions introduced by Vitek et al. 
[@vitek_repeatability_2011]:

> Repeatability. _The ability to re-run the exact same experiment with 
> the same method on the same or similar system and obtain the same or 
> very similar result._
>
> Reproducibility. _The independent confirmation of a scientific 
> hypothesis through reproduction by an independent researcher/lab. 
> The reproductions are carried out after a publication, based on the 
> information in the paper and possibly some other information, such 
> as data sets, published via scientific data repositories or provided 
> by the authors on inquiry_.

The line that divides repeatability from reproducibility can be 
subjective; from the definition of repeatability, the phrase _exact 
same experiment with the same method on the same or similar system_ 
can be a source of debate. To minimize confusion, we make it more 
concrete by defining repeatability w.r.t. the components of an 
experiment. In general, an experiment is composed of a relatively 
complex computational environment that includes one or more of the 
following: hardware, virtualization, OS, configuration, code, data and 
workload ([see\ ](#sec:means) for an extended discussion on this). We 
refer to these components as the _means_ of an experiment. For each of 
these, we can determine precisely whether they are the same as in the 
original experiment or not. We say that we repeat an experiment if and 
only if the means are the same as in the original execution. If any of 
them has changed and we re-execute the experiment, we say we are now 
trying to reproduce the results.

Version-control systems (VCS) are sometimes used to ease the 
recreation of an experimental environment. By having a particular 
version ID for the software and data used for an article's 
experimental results, reviewers and readers can have access to the 
same code base [@brown_how_2014]. However, availability of the source 
code does not guarantee reproducibility [@collberg_measuring_2014] 
since the code might not compile and, even if compilable, the 
resulting program might not be similar to the original since 
things like compilation flag ordering have an effect on the generation 
of the binary executable. Recreating an environment that resembles the 
one where an experiment was originally executed is a challenging task 
[@krishnamurthi_real_2015]. Virtualization technologies play a big 
role in accomplishing this [@clark_xen_2004 ; @jimenez_role_2015]. In 
the end, if required, the means of an experiment can be audited by 
experts in order to determine whether they resemble the original.

Regardless of whether an experiment is being repeated or reproduced, 
the results of a re-execution have to be validated. At this point, the 
reviewer has to answer the question: _"are these results validating 
the original ones?"_. An alternative validation criteria can rely on 
the exact quantitative observations, that is, a re-execution validates 
the original work if the exact same numerical values of the original 
output data are obtained. This should be applicable for an experiment 
that is being repeated since, per definition, the exact same 
experiment is executed on the same environment. However, given the 
complex and continual changes that computational environments go 
through, taking exact quantitative metrics as the basis for validation 
might be problematic, since more often than not an experiment will be 
reproduced (rather than repeated), in which case the wiggle room for 
validation is very limited. Thus, ideally, we would like to have a way 
of specifying validation criteria that is as independent as possible 
from the particular implementation details, i.e. a way of testing the 
validity of the original work that is agnostic to the means of the 
experiment. We propose to take experiment goals and observations as 
the basis for validation.

# Goals, Means and Observations

The high-level structure of an experiment can be described as having 
five components: goals, means, output data, result visualizations and 
observations (Figure 1). An experiment is designed with a particular 
goal in mind. For example, that under certain circumstances a new 
system or algorithm improves the state-of-the-art by an order of 
magnitude. The means of the experiment, as we mentioned, are the 
particularities of how the experiment is carried out. As part of the 
experiment execution, metrics are collected that are dumped into an 
output dataset. This raw data can optionally be summarized (e.g. 
obtain statistical descriptors) before being displayed in a figure and 
described in the form of observations made in the prose of the 
article. The observations that are made about the properties of the 
output data are the basis on which an author proves and corroborates 
the hypothesis of her work.

![High-level structure of an experiment.](figures/goals.png)

<!--
  * Do we need to provide examples?
  * Do we need to talk more about the schema of the raw data? E.g. 
    "These descriptions make reference to elements from the metadata 
    of the raw output data (i.e. the schema of the raw data), for 
    example, the average throughput is 10x better than the baseline".
-->

The use of declarative formats provides a researcher a way of 
expressing, in a high-level manner, the relationship between the goals 
and means of an experiment, the raw data it produces, and the figures 
and observations that are discussed in the article. In other words, it 
enables the author to provide a description of the design of an 
experiment and its means of execution as well as the expected 
observations that validate the author's claims. Such a description 
serves two purposes. On one hand, a reviewer or reader that has access 
to this description can, on her own, validate the original work by 
testing whether the original observations hold on the re-generated 
output data. On the other, the explicit specification of these 
high-level elements aid an author in enhancing the design and 
presentation of the experimental evaluation of a system.

<!--
[^claim]: We could refer to observations as "claims", but we purposely 
avoid doing so since, as should be made clear later, a claim connotes 
a strong validation criteria. In other words, if a claim is false, the 
original experiment is faulty. While an observation might be false but 
without invalidating the original result. **NOTE**: this might not be 
true. If the observation is false, it means that the re-execution 
environment is significantly distinct from the original that it 
invalidates the premises of the original.
-->

# Experiment Specification Format {#sec:format}

An experiment specification format (ESF) allows a scientist to 
explicitly capture the high-level structure of the experiment. An 
example in the JSON file format is shown below. The example 
corresponds to a simplified version of the specification of an 
already-published article ([see\ ](#sec:case)). We describe each 
section of the ESF next.

~~~{.json .numberLines}
{
"goal_location": {"sec": "6.1", "par": 5},
"goal_text": "demonstrate that Ceph scales
  linearly with the size of the cluster",
"goal_category": ["proof"],
"experiments":[ {
 "reference":"figure-8",
 "name":"scalability experiment",
 "tags":["throughput"],
 "hardware_dependencies": [ {
    "type": "hdd",
    "bw": "60MB/s"
 }],
 "software_dependencies": [ {
   "type": "os",
   "kernel": "linux",
   "kernel_version": "2.6.32",
   "distro": "debian",
   "distro_version": "6.0"
 },
 {
   "type": "storage",
   "name": "ceph",
   "version": "0.1.67"
 }],
 "independent_variables": [
 {
   "type": "method",
   "values": ["raw", "ceph"]
 },
 {
   "type": "size",
   "values": ["2-24", 2]
 }
 ],
 "dependent_variable": {
   "type":  "throughput",
   "scale": "mb/s"
 },
 "statistical_functions": {
   "functions": ["avg", "stddev"],
   "repetitions": 10
 },
 "validations": [
   "for
      size=*
    expect
      ceph = (raw * 0.9)"
]}]}
~~~

## Experiment Goals {#sec:goals}

There are many distinct types of experiment goals, i.e. the reason why 
an experiment is designed the way it is. For example:

  * _Proof of concept_. Show that the idea/algorithm/system works as 
    described. For example, an scalability experiment that shows how 
    the system scales linearly.
  * _Decision-guiding_. Determine whether a behavior is obtrusive 
    enough so that it needs to be addressed as part of the design of 
    the system. For example, test if locality of data in a NUMA-based 
    system is of significance so that the code is optimized to 
    minimize movement of data between local and remote memory banks.
  * _Robustness check_. Measure the ability of the system to react to 
    changes in the environment. For example, an experiment that 
    executes the same workload on distinct platforms and shows similar 
    behavior on all of them.
  * _Overhead quantification_. When overheads are expected, show how 
    big are they. For example, if a FUSE module is implemented, 
    measure its overhead against a kernel-level FS module.

This list is not exhaustive, of course, but it is representative of 
many experiments found in the systems research literature. The first 
elements in the ESF point to the experimental goals and link it with a 
one or more experiments in an article (lines 2-5) that are implemented 
in order to achieving them.

## Means of an Experiment {#sec:means}

<!-- **TODO**: Extend it. Talk about non-determinism -->

While computational systems are complex, advances in version-control 
and cloud computing technologies make it easier to recreate the 
environment no which an experiment runs. Immutability makes it easier 
to _pin_ a large majority of the components of an experiment as well 
as infer and package its dependencies [@chirigati_reprozip_2013]. For 
those components that cannot be fixed to a particular state, there is 
tooling that automatically obtains and formats detailed information 
about the state of the execution platform, making it easier to compare 
between original and recreated environments. The challenge lies in 
finding the root cause of the differences in original and reproduced 
results [@jimenez_redo_2014].

The ESF contains a section to specify this. In the example, this 
corresponds to lines 10-25. This is a simplified list of dependencies 
of the experiment, used only to exemplify the type of information that 
is captured in these entries. A real example would be more 
comprehensive, potentially relying on tools that obtain this 
information automatically.

<!--

The components of an experiment:

  * Hardware
  * Virtualization
  * OS
  * Configuration. OS resources made available to the experiment.
  * Code
  * Data
  * Workload
  * Non-determinism. Concurrency level, node placement, degree of 
    consolidation (how many other users are in my same node), etc.

For example, if the re-execution of an experiment that employs a 
pseudo-random number generator uses a different seed than the 
originally experiment, we can't consider this a repetition of the 
experiment, mainly due to the non-determinism introduced by the 
platform.

Another, example is the following: For example, if an experiment 
originally ran on EC2 at peak hours and now is executing at non-peak 
hours, we can't consider this a repetition of the experiment, mainly 
due to the non-determinism introduced by the platform.

Another example: if the re-execution of an experiment hosted on 
CloudLab gets assigned with distinct bare-metal resources w.r.t. the 
original execution, it's not a repetition; the underlying resources 
might have changed (e.g. moonshot vs. other type of chassis).

-->

## Schema of Raw Data {#sec:schema}

While it is important to capture the output data, making it part of 
the ESF would be cumbersome and, as has been mentioned, exact 
numerical repeatability is a very limited validation criteria. 
Instead, it is more useful to have a description of the metrics being 
captured, i.e. the metadata of the experiment's output. For example, 
if the measurements are stored in a CSV file, the experiment 
specification should include the metadata of each column such as name, 
aliases, types, scales and ranges.

The ESF has two entries for independent and dependent variables that 
are used to specify the schema of the output data (lines 26-39). The 
latter refers to the metric being captured while the former 
corresponds to the values over which the measurements were taken. 
Additionally, if statistical functions are applied to the raw data, 
these should be specified also (lines 40-43), along with the number of 
repetitions of the experiment.

## Observations and Validation Clauses {#sec:validation}

We propose using a domain specific language (DSL) for codifying 
observations. Such a language provides an author with a mechanism to 
succinctly write high-level statements that can be used to test for 
reproducibility. The simplified syntax for the language is the 
following:

```yaml
validation
 : 'for' condition ('and' condition)*
   'expect' result ('and' result)*
 ;
condition
 : vars ('in' range)
 | vars ('='|'<'|'>'|'!=') value
 ;
result
 : condition
 ;
vars
 : var (',' var)*
 ;
range
 : 'between' value 'and' value
 | '[' value (',' value)* ']'
 ;
value
 : '*' | NUMBER | var ('*') NUMBER
 ;
var
 : STRING
 ;
```

The statements constructed via this DSL refer to elements on the 
schema of the output data. In other words, the specification of the 
schema that precedes the `validations` section of the ESF introduces 
syntactic elements into the language that provide an easy way to write 
validation statements. For example, suppose there is an experiment 
that evaluates concurrency control methods and the experiment measures 
the performance of them while varying the number of worker threads. 
The schema for such an experiment might be the following:

```json
{
  "independent_variables": [ {
    "type": "method",
    "values": ["baseline", "mine"]
  }, {
    "type": "threads",
    "values": ["2", "4", "8", "16"]
  }],
  "dependent_variable": {
    "type":  "throughput",
    "scale": "ops/s"
  }
}
```

A statement for this experiment might be:

```
for
  threads > 4
expect
  mine = (10 * baseline)
```

In prose form, the above describes that when the number of worker 
threads goes beyond 4, `mine` outperforms `baseline` by an order of 
magnitude. When re-executing this experiment, the data should reflect 
this behavior in order to validate the results.

# Case Study {#sec:case}

We illustrate our approach by taking a concrete paper and describing 
the goals, means, observations and validation clauses that define the 
reproducibility criteria. We take the Ceph OSDI '06 paper 
[@weil_ceph_2006] and reproduce one of its experiments. In particular, 
we look at the scalability experiment from the data performance 
section (6.1). The reason for selecting this paper is that we are 
familiar with these experiments, making it easier to reason about 
contextual information not necessarily available directly from the 
paper.

The experiments in Section 6.1 of the original paper showed the 
ability of Ceph to saturate disk evenly among the drives of the 
cluster as well as scale with the size of the storage cluster. Results 
of the scalability experiment are presented in Section 6.1.3 of the 
Ceph paper. The goal of this experiment is to show that Ceph scales 
linearly with the number of storage nodes, up to the point where the 
network switch is saturated. This linear scalability is the validation 
criteria for this experiment and thus what we would like to capture in 
the specification.

![Reprinting Figure 8 from the original paper. The original caption 
reads: "_OSD write performance scales linearly with the 
size of the OSD cluster until the switch is saturated at 24 OSDs. 
CRUSH and hash performance improves when more PGs lower variance in 
OSD utilization_."](figures/figure8.png)

The experiment used 4 MB objects to minimize random I/O noise from the 
hard drives. We ignore the performance of the `hash` data distribution 
and increase the number of placement groups to 128 per node, thus we 
meaningfully compare against the red solid-dotted line in Figure 8 of 
the Ceph paper (Figure 8 on the original paper; reprinted in Figure 
2).

The original scalability experiment ran with 20 clients per node on 20 
nodes (400 clients total) and varied the number of OSDs from 2-26 in 
increments of 2. Every node was connected via 1 GbE link, so the 
experiment theoretical upper bound was 2GB/s (when there was enough 
capacity of the OSD cluster to have 20 1Gb connections) or 
alternatively when the connection limit of the switch was reached. The 
paper experiments were executed on a Netgear switch. This device has a 
capacity of approximately 14 GbE in _real_ total traffic (from a 20 
advertised), corresponding to the 24 * 58 = 1400 MB/s combined 
throughput shown in the original paper.

We scaled down the experiment by reducing the number of client nodes 
to 1 (running 16 client threads). This means that our network upper 
bound is approximately 110 MB/s (the capacity of the 1GbE link from 
the client to the switch). We throttle I/O at 30 MB/s, so this is our 
scaling unit (the per-OSD increment). Figure 3 shows results of this 
scaled-down, throttled version of the scalability experiment.

![Reproducing a scaled-down version of the original OSDI '06 
scalability experiment. The y-axis represents average throughput, as 
seen by the client. They x-axis corresponds to the size of the cluster 
(in number of object storage devices (OSD). The square marker 
corresponds to the average of 10 executions. The line with triangle 
markers projects the original results to our setting. This projection 
is obtained by having the 58 MB/s divided by 2 (to reflect the doubled 
I/O operation of the current Ceph version), i.e. 24 MB/s as the 
scalability unit of the original experiment.](figures/ceph.png)

The sample experiment specification shown earlier ([](#sec:format)) 
corresponds to this experiment. The validation clause in lines 45-48 
specifies the reproducibility criteria for this experiment. As can be 
noted, this is independent of the particularities of the experiment 
setup. As figure 3 shows, even though our setup is different from the 
original, we are able to reproduce the results since we validate the 
results on the basis of the schema and clauses of the experiment 
specification for this results.

# Discussion

We discuss several aspects of our proposal.

## Codified Observations As Falsifiable Statements

Validation clauses serve to succinctly codify observations. Given the 
way that the DSL is designed, validation ranges have to be provided 
for each observation so that it can be tested. This has the 
implication of turning observations into falsifiable statements 
[@popper_logic_2002]. These are conditions that should hold in order 
to validate the approach of the paper. In other words, if the means of 
the experiment are properly recreated, these conditions should be 
observed.

<!--
An example of these, respectively:

  * Ceph's scalability experiment. Ceph should scale linearly with the 
    number of OSDs regardless of the setup, as long as I/O is the 
    bottleneck.
  * The NUMA experiments of the CRUISE paper; these won't repeat on 
    setups where the memory bandwidth is the same for the entire memory 
    (in NUMA, there is the concept of local and remote banks).
  * Subjective terms as "better", "slightly", "relatively quick", etc. 
    get quantified. Although the explicit definition of a term is 
    concrete, the subjective nature of it persists. But, at least it's 
    clear what the "opinion" of the original author is in the context.
  * Brings falsifiability into picture. Having the author specify 
    validation ranges has the consequence of making the approach 
    falsifiable. This, again, might be subjective but explicit.
-->

Experiment goals ([](#sec:goals)) set the tone in which these 
falsifiable statements are treated. For an experiment that proves a 
concept or design, a failed validation clause has more weight than, 
say, an experiment that quantifies an expected overhead. For example, 
a system that claims to achieve linear scalability, the corresponding 
validation clauses is more significant than those for an experiment 
that shows the overhead of a new file system implemented as a FUSE 
module. In the former, a failed validation invalidate the whole work 
while in the latter the failed test talks about a minor aspect of the 
work. In other words, some experiments evaluate a high-level claim 
while others evaluate implementation decisions, hence the importance 
of looking at experiment goals while looking at validations; goals set 
the mindset of the reader or reviewer that validates the work.

## The Validation Workflow {#sec:workflow}

The workflow:

  1. Grab experiment, treating its means as a black box.
  2. Execute and validate by testing results against validation 
     clauses.
  3. If test fails. This is due to one of two reasons:
      1. The means of experiment are significantly different from the 
         original ones. In this case, the means of an experiment have 
         to be introspected (i.e. treated as a white/gray box now).
      2. Original work is invalid.

The structure of the ESF has the structure of a conditional statement: 
given the goals and _means_ of an experiment, the observations on the 
output data should hold. Thus, if the validation statements are false 
w.r.t. to the output data of the re-execution of an experiment, it is 
either because the differences between the means of the original and 
reproduced experiment are significantly different, or the original 
claims are false. Thus, before one can determine the latter, one has 
to audit the differences between the means of experimentation and 
account for all of them.

## Early Feedback

From exercises we have done with authors:

  * Makes you think clearly about the overall experiment design.
  * Provides a nice template for carrying out experiments.
  * Makes you find meaningful baselines. Reporting raw numbers in 
    figures and observations makes it harder to validate results. 
    Specifying validation clauses w.r.t. to baselines and normalized 
    values makes it easier to generate reproducible experiments.

## Integration to existing infrastructure

Experimental platforms such as CloudLab [@ricci_introducing_2014] can 
incorporate the notion of experiment execution so that for every 
experiment a record of executions is maintained. For each execution, 
the _means_ section of the ESF can be automatically populated. 
Validation statements can also provide another testability layer for 
continuous integration (CI) systems such as Jenkins.

# Conclusion and Future Work

In the words of Karl Popper: "_the criterion of the scientific status 
of a theory is its falsifiability, or refutability, or testability_". 
By providing a way to specify the high-level components of an 
experiment we effectively incorporate falsifiability to systems 
evaluation. We are in the process of further testing the viability of 
the ESF on experiments from other areas of systems research. As part 
of our work, we are working with colleagues in our field to create 
descriptions for already-published and going over them to see if they 
capture the validation criteria for their experiments.

# Related Work {#sec:related}

The challenging task of evaluating experimental results in applied 
computer science has been long recognized [@ignizio_establishment_1971 
; @ignizio_validating_1973 ; @crowder_reporting_1979]. This issue has 
recently received a significant amount of attention from the 
computational research community [@freire_computational_2012 ; 
@neylon_changing_2012 ; @leveqije_reproducible_2012 ; 
@stodden_implementing_2014], where the focus is more on numerical 
reproducibility rather than performance evaluation. Similarly, efforts 
such as _The Recomputation Manifesto_ [@gent_recomputation_2013] and 
the _Software Sustainability Institute_ [@crouch_software_2013] have 
reproducibility as a central part of their endeavour but leave runtime 
performance as a secondary problem. In systems research, runtime 
performance _is_ the subject of study, thus we need to look at it as a 
primary issue.

In [@collberg_measuring_2014] the authors took 613 articles published 
in 13 top-tier systems research conferences and found that 25% of the 
articles are reproducible (under their reproducibility criteria). The 
authors did not analyze performance. In our case, we are interested 
not only in being able to rebuild and execute binaries but also in 
validating the original claims by examining the characteristics of the 
output of the experiment.

Krishnamurthi and Vitek [@krishnamurthi_real_2015] emphasize the 
importance of repeatability and describe recent efforts by the systems 
research community. We see our work as complementary to these, since 
as we have exemplified, validation criteria should hold regardless of 
whether results are repeated or reproduced.

# References

<!-- hanged biblio -->

\noindent
\vspace{-2em}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
