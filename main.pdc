---
title: "Tackling the Reproducibility Problem with Declarative Experiment Specifications"
author:
  - name: "Ivo Jimenez \\and Carlos Maltzahn"
    affiliation: "University of California Santa Cruz"
    email: "`{ivo,carlosm}@cs.ucsc.edu`"
abstract: |
  Evaluating experimental results in the field of computer systems is 
  a challenging task, mainly due to the many changes in software and 
  hardware that computational environments go through. In this 
  position paper, we introduce a format for specifying. A subset of 
  this is the. We present several use cases in the area of storage 
  systems to illustrate the usefulness of this approach. We also 
  discuss its limitations and potential benefits of using this 
  approach in other areas of experimental systems research.
tags:
  - vision-paper
  - socc15
category: pubs
layout: paper
classoption:
  - 10pt
  - reprint
numbersections: true
substitute-hyperref: true
documentclass: sigplanconf
sigplanconf: true
links-as-notes: true
csl: "ieee.csl"
bibliography: "citations.bib"
fontfamily: times
usedefaultspacing: yes
keywords:
  - foo
  - bar
---

# Introduction

If you take the observation as the basis for validation, the wiggle 
room for reproducibility is very limited. We have to look at 
experiment goals instead.

What are the components of an experiment?

  * Non-determinism
  * Hardware
  * Virtualization
  * OS
  * Configuration. OS resources made available to the experiment.
  * Code
  * Data

We refer to these components as the _means_ of an experiment. We say 
we repeat an experiment iff the means are exactly the same as in the 
original. If one of them change, we say we are now reproducing an 
experimental result.

For example, if the re-execution of an experiment that employs a 
pseudo-random number generator uses a different seed than the 
originally experiment, we can't consider this a repetition of the 
experiment, mainly due to the non-determinism introduced by the 
platform.

Another, example is the following: For example, if an experiment 
originally ran on EC2 at peak hours and now is executing at non-peak 
hours, we can't consider this a repetition of the experiment, mainly 
due to the non-determinism introduced by the platform.

Another example: if the re-execution of an experiment hosted on 
CloudLab gets assigned with distinct bare-metal resources w.r.t. the 
original execution, it's not a repetition; the underlying resources 
might have changed (e.g. moonshot vs. other type of chassis).

## Goals, Means and Observations

An experiment is originally designed with the goal of proving a 
high-level claim. For example, that a new system or algorithm improves 
significantly the state-of-the-art. The means of the experiment, as we 
mentioned, are the particularities of how the experiment is carried 
out. An experimental result can be validated on the basis of the 
goals, assuming the means have been audited in order to confirm that 
they resemble the original.

Then we will show figure with goals, the individual experiments, the 
stuff with the stuff and the other things that go in the figure.

## Repeatability vs. Reproducibility

While repeatability is the ideal, in many cases it's not possible to 
attain.

  * Industrial contributions. Releasing the original code and data 
    might not be possible. Thus the "means" of the experiment aren't 
    the same
  * Infrastructure. There might be particular hardware being used that 
    it's hard to get access to.

Thus, in this work we are interested in dealing with reproducibility.

## Desiderata

We want high-level expressibility that allows a scientist to specify 
the expectations of an experiment. In other words, given the design of 
an experiment, something that enables the description of what is 
expected in terms of particular metrics

# Specification

The abstraction allows a researcher developer to specify distinct 
properties such as availability, performance, encryption, 
deduplication, compression, among others.

All of these at the level of a container. Example:

```json
{
"claims": [
  {
    "claim_location": {"section": "5.3.1", "paragraph": 4},
    "claim_text": "The results of this experiment in Figure 8 show that our modifications have the largest impact – even when not multi- executing – for applications that extensively exploit data structures, like splay and raytrace. The results also con- firm our expectations that our prototype implementation more or less doubles execution time when actively multi- executing with two security levels. The io test shows only a negligible impact overhead, because while one security level blocks on I/O, the other level can continue to execute. The results are in line with previous research results of another SME implementation",
    "claim_category": [""],
    "experiments": [
      {
        "reference": "figure-8",
        "name": "Microbenchmarks",
        "description": "",
        "tags": ["runtime"],
        "hardware_dependencies": [],
        "software_dependencies": [],
        "independent_variables": [
          {
            "type": "algorithm",
            "values": ["unmodified", "modified-without-sme", "modified-with-sme"]
          },
          {
            "type": "workload",
            "description": "Google Chrome v8 Benchmark Suite v6",
            "values": ["crypto", "deltatable", "early-boyer", "raytrace", "regexp", "richards", "splay", "io"]
          }
        ],
        "dependent_variable": { "type": "time", "scale": "n/a" },
        "statistical_functions": {
          "functions": [ "n/s" ],
          "instances": 0
        },
        "baseline": "unmodified"
      }
    ]
  },
```
# Related Work {#related}

The challenging task of evaluating experimental results in applied 
computer science has been long recognized [@ignizio_establishment_1971 
; @ignizio_validating_1973 ; @crowder_reporting_1979]. This issue has 
recently received a significant amount of attention from the 
computational research community [@freire_computational_2012 ; 
@neylon_changing_2012 ; @leveqije_reproducible_2012 ; 
@stodden_implementing_2014], where the focus is more on numerical 
reproducibility rather than performance evaluation. Similarly, efforts 
such as _The Recomputation Manifesto_ [@gent_recomputation_2013] and 
the _Software Sustainability Institute_ [@crouch_software_2013] have 
reproducibility as a central part of their endeavour but leave runtime 
performance as a secondary problem. In systems research, runtime 
performance _is_ the subject of study, thus we need to look at it as a 
primary issue. By obtaining profiles of executions and making them 
part of the results, we allow researchers to validate experiments with 
performance in mind.

<!-- hanged biblio -->

\noindent
\vspace{-2em}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
