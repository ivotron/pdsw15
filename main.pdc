---
title: "Tackling the Reproducibility Problem in Systems Research with Declarative Experiment Specifications"
author:
  - name: "Ivo Jimenez, Carlos Maltzahn"
    affiliation: "UC Santa Cruz"
    email: "`{ivo,carlosm}@cs.ucsc.edu`"
  - name: "Jay Lofstead"
    affiliation: "Sandia National Lab"
    email: "`gflofst@sandia.gov`"
  - name: "Adam Moody, Kathryn Mohror"
    affiliation: "Lawrence Livermore National Lab"
    email: "`{moody20,kathryn}@llnl.gov`"
abstract: |
  Validating experimental results in the field of computer systems is 
  a challenging task, mainly due to the many changes in software and 
  hardware that computational environments go through. Determining if 
  an experiment is reproducible entails two separate tasks: 
  re-executing the experiment and validating the results. Existing 
  reproducibility efforts have focused in the former, envisioning 
  techniques and infrastructures that makes it easier to re-execute an 
  experiment. In this work we focus on the latter by analyzing the 
  validation workflow that an experiment re-executioner goes through. 
  We notice that validating results is done on the basis of experiment 
  design and high-level goals, rather than exact quantitative metrics. 
  Based on this insight, we introduce a declarative format for 
  specifying the high-level components of an experiment as well as 
  describing generic, testable conditions that serve as the basis for 
  validation. We present a use case in the area of storage systems to 
  illustrate the usefulness of this approach. We also discuss 
  limitations and potential benefits of using this approach in other 
  areas of experimental systems research.
tags:
  - vision-paper
  - socc15
category: pubs
layout: paper
fontfamily: times
classoption:
  - 10pt
  - reprint
numbersections: true
documentclass: sigplanconf
substitute-hyperref: true
monofont-size: scriptsize
sigplanconf: true
links-as-notes: true
csl: "ieee.csl"
bibliography: "citations.bib"
usedefaultspacing: yes
keywords:
  - reproducibility
---

# Introduction

A key component of the scientific method is the ability to revisit and 
replicate previous experiments. Registering detailed information about 
how an experiment was carried out allows scientists to interpret and 
understand results, as well as verify that the experiment was 
performed according to acceptable procedures. Additionally, 
reproducibility plays a major role in education since the amount of 
information that a student has to digest increases as the pace of 
scientific discovery accelerates. By having the ability to reproduce 
experiments, a student can learn by looking at provenance information, 
re-evaluate the questions that the original experiment answered and 
thus "stand on the shoulder of giants".

In applied computer science an experiment is carried out in its 
entirety on a computer. Thus, in principle, a well documented 
experiment should be reproducible automatically (e.g. by typing 
`make`); however, this is not the case. Today's computational 
environments are complex and accounting for all possible effects of 
changes within and across systems is a challenging task. This and 
other related issues have gained substantial attention lately in 
systems research [@vitek_repeatability_2011 ; 
@collberg_repeatability_2015 ; @dietrich_dataref_2015 ; 
@feitelson_repeatability_2015], computational science 
[@neylon_changing_2012 ; @peng_reproducible_2011 ; 
@freire_computational_2012 ; @donoho_reproducible_2009] and science in 
general [@achenbach_new_2015 ; @yaffe_reproducibility_2015 ; 
@editorial_journals_2014]. The role of computers in new scientific 
discoveries has only increased and will keep doing so. Coupled with 
this, the advent of cloud computing makes it easier to share code and 
data, making it straight-forward to collaborate in the implementation 
of experiments. In this scenario, there is a serious lack of tools 
that allow researchers to identify when an experiment is generating 
valid results. So while it is becoming easier to collaborate, the same 
cannot be said about experiment validation. The goal of our work is to 
close this gap in the area of systems research and storage systems in 
particular.

When discussing reproducibility, the terms _reproducibility_, 
_repeatability_, _replicability_ and _recomputability_ (among many 
others) are often used, sometimes interchangeably. In our work we only 
employ _repeatability_ and _reproducibility_. We borrow and refine the 
definitions introduced by Vitek et al. [@vitek_repeatability_2011]:

> Repeatability. _The ability to re-run the exact same experiment with 
> the same method on the same or similar system and obtain the same or 
> very similar result._
>
> Reproducibility. _The independent confirmation of a scientific 
> hypothesis through reproduction by an independent researcher/lab. 
> The reproductions are carried out after a publication, based on the 
> information in the paper and possibly some other information, such 
> as data sets, published via scientific data repositories or provided 
> by the authors on inquiry_.

The reproduction of an experiment can be seen as composed by (1) its 
execution and (2) the validation of the results. Colloquially, these 
two tasks are conflated when designating an experiment as 
reproducible. In our case, we treat re-execution and validation 
separately since it facilitates the discussion and makes it easer to 
identify the missing pieces in terms of a generic, systematic approach 
to reproducibility.

Regarding re-execution, the line that divides repeatability from 
reproducibility can be subjective; from the definition of 
repeatability, the phrase _exact same experiment with the same method 
on the same or similar system_ can be a source of debate. To minimize 
confusion, we concretize it by defining repeatability w.r.t. the 
components of an experiment. In general, an experiment is composed of 
a relatively complex computational environment that includes one or 
more of the following: hardware, virtualization, OS, configuration, 
code, data and workload ([see\ ](#sec:means)). We refer to these 
components as the _means_ of an experiment. For each of these, we can 
determine precisely whether they are the same as in the original 
experiment or not. We say that we repeat an experiment if and only if 
the means are the same as in the original execution. If any of them 
has changed and we re-execute the experiment, we say we are now trying 
to reproduce the original execution.

Version-control systems (VCS) are sometimes used to ease the 
recreation of an experimental environment. Having a specific version 
ID for the source code and data associated to an article on a public 
repository reduces significantly the burden of recreating the means of 
an experiment [@brown_how_2014]. However, availability of the source 
code does not guarantee reproducibility [@collberg_repeatability_2015] 
since the code might not compile and, even if compilable, the 
resulting program might not generate the same results. Recreating an 
environment that resembles the one where an experiment was originally 
executed is a challenging task [@krishnamurthi_real_2015]. 
Virtualization technologies play a big role in accomplishing this 
[@clark_xen_2004 ; @jimenez_role_2015]. In the end, if required, the 
means of an experiment can be audited by experts in order to determine 
whether they resemble the original or not ([](#sec:workflow)).

The second step in determining the reproducibility of an experiment is 
validating the results of an execution. Regardless of whether an 
execution is repeated or reproduced, at this point, the reviewer has 
to answer the question: _"are these results validating the original 
ones?"_. An alternative validation criteria can rely on the exact 
quantitative observations, that is, results validate the original work 
if the exact same numerical values of the original output are 
obtained. This should be applicable for an experiment that is being 
repeated since, per definition, the exact same experiment is executed 
on the same environment. However, given the complex and continual 
changes that computational environments go through, taking exact 
quantitative metrics as the basis for validation might be problematic, 
since more often than not an experiment execution will be reproduced 
(rather than repeated), in which case the wiggle room for validation 
is very limited. Thus, ideally, we would like to have a way of 
specifying validation criteria that is as independent as possible from 
the particular implementation details, i.e. a way of testing the 
validity of the original work that is agnostic to the means of the 
experiment. We propose to take experiment goals as the basis for 
validation and treat observations in the context of these goals.

# Goals, Means and Observations

The high-level structure of an experiment can be described as having 
five components: goals, means, output data, result visualizations and 
observations (Figure 1). An experiment is designed with a particular 
goal in mind. For example, that under certain circumstances a new 
system or algorithm improves the state-of-the-art by an order of 
magnitude. The means of the experiment, as we mentioned, are the 
particularities of how the experiment is carried out. As part of the 
experiment execution, metrics are collected that are dumped into an 
output dataset. This raw data can optionally be summarized (e.g. 
obtain statistical descriptors) before being displayed in a figure and 
described in the form of observations made in the prose of the 
article. The observations that are made about the properties of the 
output data are the basis on which an author proves and corroborates 
the hypothesis of her work.

![High-level structure of an experiment.](figures/goals.png)

<!--
  * Do we need to provide examples?
  * Do we need to talk more about the schema of the raw data? E.g. 
    "These descriptions make reference to elements from the metadata 
    of the raw output data (i.e. the schema of the raw data), for 
    example, the average throughput is 10x better than the baseline".
-->

The use of declarative formats provides a researcher a way of 
expressing, in a high-level manner, the relationship between the goals 
and means of an experiment, the raw data it produces, and the figures 
and observations that are discussed in the article. In other words, it 
enables the author to provide a description of the design of an 
experiment and its means of execution as well as the expected 
observations that validate the author's claims. Such a description 
serves two purposes. On the one hand, a reviewer or reader that has 
access to this description can, on her own, validate the original work 
by testing whether the original observations hold on the re-generated 
output data. On the other, the explicit specification of these 
high-level elements aid an author in enhancing the design and 
presentation of the experimental evaluation of a system.

# Experiment Specification Format {#sec:format}

An experiment specification format (ESF) allows a scientist to 
explicitly and declaratively capture the high-level structure of an 
experiment. An example in the JSON file format is shown below. The 
example corresponds to a simplified version of the specification of an 
already-published article ([see\ ](#sec:case)). We describe each 
section of the ESF next.

~~~{.json .numberLines}
{
  "goal_location": {
     "sec": "6.1",
     "par": 5
  },
  "goal_text": "demonstrate that Ceph scales linearly
     with the size of the cluster",
  "goal_category": ["proof"],
  "experiments":[ {
     "reference":"figure-8",
     "name":"scalability experiment",
     "tags":["throughput"],
     "hardware_dependencies": [{
        "type": "hdd",
        "bw": "58MB/s"
     }],
     "software_dependencies": [{
       "type": "os",
       "kernel": "linux 2.6.32",
       "distro": "debian 6.0"
     }, {
       "type": "storage",
       "name": "ceph",
       "version": "0.1.67"
     }],
     "independent_variables": [{
       "type": "method",
       "values": ["raw", "ceph"],
       "desc": "raw corresponds to hdd sequential write
          performance, expressed in MB/s"
     }, {
       "type": "size",
       "values": ["2-24", 2]
     }],
     "dependent_variable": {
       "type":  "throughput",
       "scale": "mb/s"
     },
     "statistical_functions": {
       "functions": ["avg", "stddev"],
       "repetitions": 10
     },
     "validations": [
       "for    size=*
        expect ceph >= (raw * 0.9)"
     ]
  }]
}
~~~

## Experiment Goals {#sec:goals}

There are many distinct types of experiment goals, i.e. the reason why 
an experiment is designed the way it is. For example:

  * _Proof of concept_. Show that the idea/algorithm/system works as 
    described. For example, an scalability experiment that shows how 
    the system scales linearly.
  * _Decision-guiding_. Determine whether a behavior is obtrusive 
    enough so that it needs to be addressed as part of the design of 
    the system. For example, test if locality of data in a NUMA-based 
    system is of significance so that the code is optimized to 
    minimize movement of data between local and remote memory banks.
  * _Robustness check_. Measure the ability of the system to react to 
    changes in the environment. For example, an experiment that 
    executes the same workload on distinct platforms and shows similar 
    behavior on all of them.
  * _Overhead quantification_. When overheads are expected, show how 
    big they are for a particular setup. For example, if a FUSE driver 
    is implemented, measure its overhead against a kernel-level file 
    system module.

This list is not exhaustive, of course, but it is representative of 
many experiments found in the systems research literature. The first 
elements in the ESF specify the experimental goal and link it with one 
or more experiments that appear in the article (lines 2-5) that serve 
to accomplish the goal.

## Means of an Experiment {#sec:means}

<!-- **TODO**: Extend it. Talk about non-determinism -->

While computational systems are complex, advances in version-control 
and cloud computing technologies make it easier to recreate the 
environment on which an experiment runs. Immutability makes it easier 
to _pin_ a large majority of the components of an experiment as well 
as infer and package its dependencies [@chirigati_reprozip_2013]. For 
those components that cannot be fixed to a particular state, there is 
tooling that automatically obtains and formats detailed information 
about the state of the execution platform, making it easier to compare 
between original and re-execution environments. The challenge lies in 
finding, when present, the root cause of the differences in original 
and reproduced results [@jimenez_redo_2014].

The ESF contains a section to specify the means of the experiment. In 
the example, this corresponds to lines 10-23. This is a simplified 
list of dependencies for the experiment, used only to illustrate the 
type of information that is captured in this section. A real example 
would be more comprehensive, potentially relying on tools that obtain 
this information [automatically](https://github.com/sosreport/sos).

<!--

The components of an experiment:

  * Hardware
  * Virtualization
  * OS
  * Configuration. OS resources made available to the experiment.
  * Code
  * Data
  * Workload
  * Non-determinism. Concurrency level, node placement, degree of 
    consolidation (how many other users are in my same node), etc.

For example, if the re-execution of an experiment that employs a 
pseudo-random number generator uses a different seed than the 
originally experiment, we can't consider this a repetition of the 
experiment, mainly due to the non-determinism introduced by the 
platform.

Another, example is the following: For example, if an experiment 
originally ran on EC2 at peak hours and now is executing at non-peak 
hours, we can't consider this a repetition of the experiment, mainly 
due to the non-determinism introduced by the platform.

Another example: if the re-execution of an experiment hosted on 
CloudLab gets assigned with distinct bare-metal resources w.r.t. the 
original execution, it's not a repetition; the underlying resources 
might have changed (e.g. moonshot vs. other type of chassis).

-->

## Schema of Raw Data {#sec:schema}

While it is important to capture the output data, making it part of 
the ESF would be cumbersome and, as has been mentioned, exact 
numerical repeatability is a very limited validation criteria. 
Instead, it is preferable to have a description of the metrics being 
captured, i.e. the metadata of the experiment's output. For example, 
if the measurements are stored in a CSV file, the experiment 
specification should include the metadata of each column such as name, 
aliases, types, scales and ranges.

The ESF has two entries for independent and dependent variables that 
are used to specify the schema of the output data (lines 24-37). The 
latter refers to the metric being captured while the former 
corresponds to the values over which the measurements are taken. 
Additionally, if statistical functions are applied to the raw data, 
these should also be specified (lines 38-41), along with the number of 
repetitions of the experiment.

## Observations and Validation Clauses {#sec:validation}

We propose using a domain specific language (DSL) for codifying 
observations. Such a language provides an author with a mechanism to 
succinctly write declarative statements that can be used to test for 
reproducibility. The simplified syntax for the language is the 
following:

```yaml
  validation
   : 'for' condition ('and' condition)*
     'expect' result ('and' result)*
   ;
  condition
   : vars ('in' range)
   | vars ('='|'<'|'>'|'!=') value
   ;
  result
   : condition
   ;
  vars
   : var (',' var)*
   ;
  range
   : 'between' value 'and' value
   | '[' value (',' value)* ']'
   ;
  value
   : '*' | NUMBER | var '*' NUMBER
   ;
  var
   : STRING
   ;
```

The statements constructed via this DSL refer to elements on the 
schema of the output data. In other words, the specification of the 
schema that precedes the `validations` section of the ESF introduces 
syntactic elements into the language that provide an easy way to write 
validation statements. For example, suppose there is an experiment 
that evaluates concurrency control methods and the experiment measures 
the performance of them while varying the number of worker threads. 
The schema for such an experiment might be the following:

```json
  {
    "independent_variables": [ {
      "type": "method",
      "values": ["baseline", "mine"]
    }, {
      "type": "threads",
      "values": ["2", "4", "8", "16"]
    }],
    "dependent_variable": {
      "type":  "throughput",
      "scale": "ops/s"
    }
  }
```

A statement for this experiment might be:

```sql
  for    threads > 4
  expect mine = (10 * baseline)
```

In prose form, the above describes that when the number of worker 
threads goes beyond 4, `mine` outperforms `baseline` by an order of 
magnitude. When re-executing this experiment, the data should reflect 
this behavior in order to validate the results.

# Case Study {#sec:case}

We illustrate our approach by taking a concrete paper and describing 
the goals, means, observations and validation clauses that define the 
reproducibility criteria. We take the Ceph OSDI '06 paper 
[@weil_ceph_2006] and reproduce one of its experiments. In particular, 
we look at the scalability experiment from the data performance 
section (6.1 on the original paper). The reason for selecting this 
paper is that we are familiar with these experiments, making it easier 
to reason about contextual information not necessarily available 
directly from the paper.

The experiments in Section 6.1 of the original paper showed the 
ability of Ceph to saturate disk evenly among the drives of the 
cluster as well as scale with the size of the storage cluster. Results 
of the scalability experiment are presented in Section 6.1.3 of the 
Ceph paper. The goal of this experiment is to show that Ceph scales 
linearly with the number of storage nodes, up to the point where the 
network switch is saturated. This linear scalability is the validation 
criteria for this experiment and thus what we would like to capture in 
the specification.

The experiment used 4 MB objects to minimize random I/O noise from the 
hard drives. We ignore the performance of the `hash` data distribution 
and increase the number of placement groups to 128 per node, thus we 
meaningfully compare against the red solid-dotted line in Figure 8 of 
the Ceph paper (reprinted in Figure 2). The original scalability 
experiment ran with 20 clients per node on 20 nodes (400 clients 
total) and varied the number of object storage devices (OSDs) from 
2-26 in increments of 2. Every node was connected via 1 GbE link, so 
the experiment theoretical upper bound was 2GB/s (when there was 
enough capacity of the OSD cluster to have 20 1Gb connections) or 
alternatively when the connection limit of the switch was reached. The 
paper experiments were executed on a Netgear switch. This device has a 
capacity of approximately 14 GbE in _real_ total traffic (from a 20 
advertised), corresponding to the 24 * 58 = 1400 MB/s combined 
throughput shown in the original paper.

![Reprinting Figure 8 from the original paper. The original caption 
reads: "_object storage device (OSD) write performance scales linearly 
with the size of the OSD cluster until the switch is saturated at 24 
OSDs. CRUSH and hash performance improves when more PGs lower variance 
in OSD utilization_."](figures/figure8.png)

We scaled down the experiment by reducing the number of client nodes 
to 1 (running 16 client threads). This means that our network upper 
bound is approximately 110 MB/s (the capacity of the 1GbE link from 
the client to the switch). We throttle I/O at 15 MB/s for each storage 
node [^throttle], so this is our scaling unit (the per-OSD increment). 
Figure 3 shows results of this scaled-down, throttled version of the 
scalability experiment.

[^throttle]: We throttle I/O with the purpose of slowing down the 
experiment. The hard drives used for the reproduced experiment can 
perform at 120 MB/s which would saturate the network link rapidly.

We see that Ceph scales linearly with the number of OSDs, up to the 
point where we saturate the 1GbE link of the client (at 8 OSDs). The 
sample experiment specification shown earlier ([](#sec:format)) 
corresponds to this experiment. The validation clause in lines 42-47 
specifies the reproducibility criteria for this experiment. As can be 
noted, this is where the declarative nature of the specification 
stands out since the validation is independent of the particularities 
of the means of each experiment. Even though the recreated environment 
is significantly different from the original[^setup], we are able to 
reproduce the results by validating on the basis of the experiment 
goal, schema of the output and validation clauses (which in turn are 
expressed as relative rather than absolute throughput measurements).

[^setup]: Besides scaling down and throttling I/O, the specifications 
of the underlying hardware are significantly different. The original 
experiment ran on nodes with a 2-core 2212 AMD Opteron @2.0GHz, 8GB 
RAM, Linux 2.6.9, 250GB Seagate Barracuda ES (ST3250620NS) hard disk 
drive and a very early version of the Ceph codebase (commit done in 
2005). The reproduced experiment ran on nodes with a 6-core Xeon 
E5-2630 @2.3GHz, 64GB RAM, 500GB HP6G (658071-B21) hard disk drive and 
Ceph version 0.87.1. The network switch was the same used for the two 
experiments, a Netgear GS748T. The complete platform specs are 
contained in the repository associated to this paper 
<https://github.com/ivotron/socc15>

![Reproducing a scaled-down version of the original OSDI '06 
scalability experiment. The x-axis corresponds to the size of the 
cluster (in number of OSDs). The y-axis represents normalized 
throughput (to meaningfully compare against original results) w.r.t. 
the raw performance of the hard disk drives in the cluster. The red 
line corresponds to the original results and the green line to the one 
generated by the re-execution of the experiment.](figures/ceph.png)

# Discussion

We discuss several aspects of our proposal.

## Usability

Given that the high-level components abstract a large number of 
experiments that people usually implement in the systems literature, 
and since this is what a researcher usually goes through anyway, 
creating the specification for an experiment represents little extra 
effort, with the exception of including information about the means of 
the experiment which, as we mentioned before ([](#sec:means)), it is a 
task that can be automated using tools that are currently available.

## Integration into Existing Infrastructure

Experimental platforms such as CloudLab [@ricci_introducing_2014] can 
incorporate the notion of _execution_ so that for every experiment a 
record of executions is maintained. For each execution, the means 
section of the ESF can be automatically populated. Validation 
statements can also provide another testability layer for continuous 
integration (CI) systems such as Jenkins.

## Codified Observations As Falsifiable Statements

Validation clauses serve to succinctly codify observations. Given the 
way that the DSL is designed, validation ranges have to be provided 
for each observation so that it can be tested. This has the 
implication of turning observations into falsifiable statements 
[@popper_logic_2002]. These are conditions that should hold in order 
to validate the conclusions of the paper. In other words, if the means 
of the experiment are properly recreated, these conditions should be 
observed.

Experiment goals ([](#sec:goals)) set the tone in which these 
falsifiable statements are treated. For an experiment that proves a 
concept or design, a validation clause has more weight than, say, an 
experiment that quantifies an expected overhead. For example, a system 
that claims to achieve linear scalability, the corresponding 
validation clauses are more significant than those for an experiment 
that shows the overhead of a new file system implemented as a FUSE 
module. In the former, a failed validation invalidates the whole work 
while in the latter the failed test invalidates a minor aspect. In 
other words, some experiments evaluate a high-level claim while others 
refer to low-level aspects, hence the importance of looking at 
experiment goals while looking at validations; goals set the mindset 
of the reader or reviewer that validates the work whenever she 
encounters failed validations. This is the main motivation for having 
goals as an explicit entry on the ESF.

## The Validation Workflow {#sec:workflow}

The ESF has the structure of a conditional statement: given the goals 
and means of an experiment, the observations on the output data should 
hold. Thus, if the validation statements are false w.r.t. the output 
data of the re-execution of an experiment, it is either because the 
differences between the means of the original and reproduced 
experiment are significantly different, or the original claims cannot 
be corroborated. Thus, before one can determine the latter, one has to 
audit the differences between the means of experimentation and account 
for all of them (Figure 4).

![Validation workflow.](figures/workflow.png)

## Early Feedback

The following are quotes from authors that have kindly worked with us 
by creating specifications for one or more of their published 
experiments:

> Author 1: _Writing an experiment specification makes you think 
> clearly about the overall experiment design_.

> Author 2: _The ESF provides a nice template for carrying out 
> experiments_.

> Author 3: _This approach helps to find meaningful baselines. 
> Reporting raw numbers in figures and observations makes it harder to 
> validate results. Specifying validation clauses respective to 
> baselines and normalized values makes it easier to report 
> reproducible results_.

In general, we have noticed that the exercise of explicitly specifying 
the validation criteria creates a feedback loop in an author's mind 
that results in insightful ideas when it comes to experiment design, 
baseline selection and validation criteria.

# Conclusion and Future Work

In the words of Karl Popper: "_the criterion of the scientific status 
of a theory is its falsifiability, or refutability, or testability_". 
By providing a way to specify the high-level components of an 
experiment and validation clauses for observed metrics we effectively 
incorporate falsifiability to the field of experimental storage 
systems. We are in the process of studying the viability of the ESF on 
experiments from other areas of systems research. As part of our work, 
we are working with colleagues in our field to create descriptions for 
already-published experiments and analyze them to check if they 
capture the appropriate validation criteria. While we envision our 
findings to be applicable in the area of systems research, we plan to 
evaluate its suitability on other areas of computer science.

# Related Work {#sec:related}

The challenging task of evaluating experimental results in applied 
computer science has been long recognized [@ignizio_establishment_1971 
; @ignizio_validating_1973 ; @crowder_reporting_1979]. This issue has 
recently received a significant amount of attention from the 
computational research community [@freire_computational_2012 ; 
@neylon_changing_2012 ; @leveqije_reproducible_2012 ; 
@stodden_implementing_2014], where the focus is more on numerical 
reproducibility rather than performance evaluation. Similarly, efforts 
such as _The Recomputation Manifesto_ [@gent_recomputation_2013] and 
the _Software Sustainability Institute_ [@crouch_software_2013] have 
reproducibility as a central part of their endeavour but leave 
performance as a secondary problem. In systems research, performance 
_is_ the subject of study, thus we need to look it as a primary issue.

In [@collberg_repeatability_2015] the authors took 601 articles 
published in 13 top-tier systems research conferences and found that 
32.3% of associated experiments are repeatable (under their _weak 
repeatability_ criteria). The authors did not validate the original 
results. In our case, we are interested not only in being able to 
rebuild and execute binaries (repeat/reproduce the execution) but also 
in validating the original claims by testing falsifiable statements on 
the output of the experiment.

Krishnamurthi and Vitek [@krishnamurthi_real_2015] emphasize the 
importance of repeatability and describe recent efforts by the systems 
research community to encourage the submission of experiment artifacts 
as assets associated to an article. We see our work as complementary 
to these since an experiment specification could also be part of this 
list of assets, making it easier to validate a re-generated result.

# References

<!-- hanged biblio -->

\noindent
\vspace{-2em}
\setlength{\parindent}{-0.26in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
